# -*- coding: utf-8 -*-
"""ho_feature_engineering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xwl01eFuQgTgfipRB8pmIojHBtlTToDD

Import library yang dibutuhkan
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, RobustScaler, StandardScaler
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_curve, precision_recall_curve
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import spearmanr
from sklearn.feature_selection import mutual_info_classif
import warnings
warnings.filterwarnings('ignore')
pd.set_option("display.max_columns", None)

"""akses ke google drive """

from google.colab import drive
drive.mount('/drive/', force_remount=True)

cd /drive/My\ Drive/file_DigiXed/tugas

"""(1) Download dataset pada link : https://drive.google.com/file/d/10sQwOFb9aE95GLGV1KIcO3zAvcHLPBy5/view?usp=sharing

(2) Upload di drive masing-masing

(3) gunakan data churn.csv, drop kolom yang tidak dibutuhkan.

(4) Define target/output = "Attrition_Flag", feature/input kolom lainnya

(5) Split data menjadi training set dan test set (test_size = 0.2) dengan stratify pada kolom **Attrition_Flag**  (1=yes, 0=no)

(6) print shape pada X_train, X_test, y_train, dan y_test
"""

churn = pd.read_csv('churn.csv')

churn.shape

churn.head()

churn_clean = churn.drop(['CLIENTNUM'], axis = 1)

churn_clean.head()

churn['Attrition_Flag'].unique()

churn_clean['Attrition_Flag'] = churn_clean['Attrition_Flag'].replace({'Existing Customer': 0, 'Attrited Customer': 1})

churn_clean.head()

X = churn_clean.drop('Attrition_Flag', axis = 1)
y = churn_clean['Attrition_Flag']

"""(5) Split data menjadi training set dan test set (test_size = 0.2) dengan stratify pada kolom **Attrition_Flag**  (1=yes, 0=no)

(6) print shape pada X_train, X_test, y_train, dan y_test
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

"""# Imbalance Data Handling

(1) Cek nilai sebaran output/target

(2) lakukan imbalance data handling (oversampling atau undersampling)

(3) fit_resample dan dapatkan dataset baru yang dengan nilai output yang balance
"""

y_train.value_counts()

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state = 42)

X_ros, y_ros = ros.fit_resample(X_train, y_train)

y_ros.value_counts()

X_train = X_train
y_train = y_train

"""# Imputation

(1) Cek missing value pada x_train

(2) lakukan imputation dengan menggunakan KNNImputer (jika perlu)
"""

X_train.isna().sum()/len(X_train)

X_train_cat = X_train.select_dtypes(include = 'object')
X_train_num = X_train.select_dtypes(include = 'number')

X_train_cat.head()

X_train_num.head()

oe = OrdinalEncoder()

X_train_cat_oe = pd.DataFrame(oe.fit_transform(X_train_cat), columns = oe.feature_names_in_)

X_train_cat_oe.head()

X_train_oe = pd.concat([X_train_cat_oe, X_train_num.reset_index(drop = True)], axis = 1)

X_train_oe.head()

knn_imputer = KNNImputer()

X_train_oe_imp = pd.DataFrame(knn_imputer.fit_transform(X_train_oe),
                              columns = knn_imputer.feature_names_in_)

X_train_oe_imp.isna().sum()

X_train_oe_imp[oe.feature_names_in_] = oe.inverse_transform(X_train_oe_imp[oe.feature_names_in_])

"""# Transformation

(1) Lakukan transformation :
- data number pake scalling
- data categorical pake OHE

(2) Pada kolom kategorikal X_train lakukan OHE pada parameter handle_unknown beri argumen 'ignore' agar encoder robust terhadap adanya kategori di testing set yang tidak ada di training set.
"""

X_train_oe_imp.head()

X_train_imp_num = X_train_oe_imp.select_dtypes(include='number')
X_train_imp_cat = X_train_oe_imp.select_dtypes(include='object')

ohe = OneHotEncoder(handle_unknown = 'ignore')

X_train_imp_cat.head()

X_train_cat_ohe = pd.DataFrame(ohe.fit_transform(X_train_imp_cat).toarray(),
                               columns = ohe.get_feature_names_out(X_train_imp_cat.columns))

X_train_cat_ohe.head()

rs = RobustScaler()

X_train_num_scaled = pd.DataFrame(rs.fit_transform(X_train_imp_num), columns = rs.feature_names_in_)

X_train_num_scaled.describe()

X_train_transformed = pd.concat([X_train_cat_ohe, X_train_num_scaled], axis = 1)

X_train_transformed.head()

"""# Feature Selection

(1) Lakukan feature selection menggunakan multicolinearity reduction dan mutual info.
"""

corr = X_train_transformed.corr(method = 'spearman')

plt.figure(figsize=(25,25))
sns.heatmap(corr, annot=True)

var = []
drop = []
for x in X_train_transformed.columns:
  for y in X_train_transformed.columns:
    if x != y:
      if [y,x] not in var:
        corr, p_value = spearmanr(X_train_transformed[x], X_train_transformed[y])
        var.append([x, y])
        if (corr <= -0.7) | (corr >= 0.7):
          if p_value < 0.05:
            drop.append(y)

drop

X_train_corr = X_train_transformed.drop(drop, axis = 1)

X_train_corr.shape

X_train_transformed.shape

mutual_info_classif(X_train_corr,
                    y_train,
                    random_state = 42)

mutual_table = pd.DataFrame(mutual_info_classif(X_train_corr,y_train,random_state = 42),
                            index = X_train_corr.columns,
                            columns = ['mutual_info']).sort_values('mutual_info', ascending = False)

mutual_table

final_col = mutual_table.iloc[0:17].index

final_col

X_train_final = X_train_corr.loc[:, final_col]

X_train_final.shape

X_train_final.head()